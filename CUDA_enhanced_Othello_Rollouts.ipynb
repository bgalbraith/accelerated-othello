{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUDA-enhanced Othello Rollouts",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYzhlckesSQYbn6qjxGkcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalbraith/accelerated-othello/blob/main/CUDA_enhanced_Othello_Rollouts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huH__MYe70FB"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This is an initial pass at implementing massively parallel support for MCTS experiments using the game Othello.\n",
        "\n",
        "It is based on custom CUDA kernels implemented via Numba's CUDA JIT capabilities.\n",
        "\n",
        "Currently, the code does the following:\n",
        "Given a number of games to evaluate in parallel, a starting board configuration, and the player who's turn it is, play all the games out to completion by taking random actions for each player. The final output is a vector indicating which player won each game or if it was a draw."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuH5wxFyD9pg"
      },
      "source": [
        "import os\n",
        "\n",
        "from numba import jit, cuda, float32, int32\n",
        "from numba.cuda.random import create_xoroshiro128p_states, xoroshiro128p_uniform_float32\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0guOT7ex8_eV"
      },
      "source": [
        "## Setting up Numba for CUDA\n",
        "\n",
        "In order for Numba to JIT compile CUDA kernels, it needs to know where the appropriate system libraries are. This is done using environment variables. The below values should be safe for Google Colab, but if an error occurs when attempting to compile, running the following in a cell will give you an idea of which paths to use:\n",
        "```\n",
        "!find / -iname 'libdevice'\n",
        "!find / -iname 'libnvvm.so'\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5Du8HNlERtL"
      },
      "source": [
        "os.environ['NUMBAPRO_LIBDEVICE'] = \"/usr/local/cuda-10.0/nvvm/libdevice\"\n",
        "os.environ['NUMBAPRO_NVVM'] = \"/usr/local/cuda-10.0/nvvm/lib64/libnvvm.so\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llkSnCc_98Yv"
      },
      "source": [
        "We set the number of games as a constant here. We also use a ray-casting strategy for finding valid moves in Othello, so we establish the eight possible ray directions here as well. Note the dtype is set to `np.int32`. This is because we will be loading this onto the GPU later, and it's better to use 32-bit data types (`int32`, `float32`) for GPU ops unless you really need the higher precision."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBB831jAEftR"
      },
      "source": [
        "N_GAMES = 32\n",
        "RAYS = np.array([[0, 1],  # east\n",
        "                 [0, -1],  # west\n",
        "                 [1, 0],  # south\n",
        "                 [-1, 0],  # north\n",
        "                 [1, 1],  # southeast\n",
        "                 [1 , -1],  # southwest\n",
        "                 [-1, 1],  # northeast\n",
        "                 [-1, -1]  # northwest\n",
        "                 ], dtype=np.int32)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee3te4gE-pQW"
      },
      "source": [
        "## CUDA Device Function: Ray Casting\n",
        "A device function is a utility function meant to be called by CUDA other kernels. It doesn't have implicit access to the thread that is calling it, but it is allowed to return a value (kernels must return None).\n",
        "\n",
        "In this case, the ray casting function is implemented as a device function as we need to use it in two different kernel calls. Its purpose is to determine if a ray cast in a particular direction results in a valid hit -- that there is a continuous line of opposing player tokens that ends in the current player's token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVoms9nrrwyx"
      },
      "source": [
        "@cuda.jit(device=True)\n",
        "def ray_cast(board, x0, y0, ray, player):\n",
        "  opponent = 3 - player\n",
        "  x = x0 + ray[0]\n",
        "  y = y0 + ray[1]\n",
        "  \n",
        "  if x < 0 or y < 0 or x >= 8 or y >= 8 or board[x, y] != opponent:\n",
        "    return False\n",
        "    \n",
        "  x += ray[0]\n",
        "  y += ray[1]\n",
        "  while x >= 0 and y >= 0 and x < 8 and y < 8:\n",
        "    if board[x, y] == 0:\n",
        "      return False\n",
        "    \n",
        "    if board[x, y] == player:    \n",
        "      return True\n",
        "    \n",
        "    x += ray[0]\n",
        "    y += ray[1]\n",
        "\n",
        "  return False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts2n2a-y_8MR"
      },
      "source": [
        "## CUDA Kernel: Find Action Probabilities\n",
        "The first part of a player's turn is to find all the valid actions available to it. In the single-threaded solution, candidate positions were selected and then each one was sequentially checked by casting rays to determine if any resulted in a hit.\n",
        "\n",
        "**A note on CUDA kernels**: As these functions cannot return values directly, we have to pass in references to arrays that we can write results to and then refer to later in the processing pipeline. In this case, the output array is called `action_probs` for reasons we will get to shortly.\n",
        "\n",
        "In the CUDA SIMT model, we are going to have each thread evaluate a single position in each game by setting our thread grid to have n_blocks = N_GAMES and each block be an 8x8 array of threads. Here we just have each thread cast rays from its assigned spot. If the current spot isn't free, that thread can stop immediately and store a 0 for that position. If we do get a ray hit, we store a positive value for that position and stop. Finally, if no hits are observed, we also store a 0.\n",
        "\n",
        "### Why action probabilities and not just booleans?\n",
        "\n",
        "Choosing a random element from a list is trivial with numpy but is not trivial inside a CUDA kernel for a few reasons. First, to get the most speedup, you need to avoid transferring data between the GPU and host, so doing as much as possible in kernels is desirable. Second, we have multiple threads all executing concurrently but nondeterministic in time, so some kind of coordinated reduction strategy is needed to first isolate valid actions and then choose amongst them. The strategy here is to make that choice random, which adds additional complexity as now we need a source for RNG and a way to use that efficiently to pick an option. The approach I have come up with is to assign each position a probability score given by\n",
        "\n",
        "$$\\mathbb{1}_{[hit]} \\cdot  p(position|board), p \\sim \\mathcal{U}(0,1).$$\n",
        "\n",
        "We can then use a greedy strategy (i.e., an argmax) to select our random action, which we shall see in the next kernel function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8x8XNwA6E4hW"
      },
      "source": [
        "@cuda.jit\n",
        "def find_actions(rng_states, boards, player, action_probs):\n",
        "  rays = cuda.const.array_like(RAYS)\n",
        "\n",
        "  tx = cuda.threadIdx.x\n",
        "  ty = cuda.threadIdx.y\n",
        "  bx = cuda.blockIdx.x\n",
        "  tid = cuda.grid(1)\n",
        "  \n",
        "  # space isn't empty, invalid move\n",
        "  if boards[bx, tx, ty] != 0:\n",
        "    action_probs[bx, tx, ty] = 0\n",
        "    return\n",
        "\n",
        "  opponent = 3 - player\n",
        "  for i in range(8):\n",
        "    hit = ray_cast(boards[bx], tx, ty, rays[i], player)\n",
        "    if hit:\n",
        "      action_probs[bx, tx, ty] = xoroshiro128p_uniform_float32(rng_states, tid)\n",
        "      return\n",
        "\n",
        "  action_probs[bx, tx, ty] = 0"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMVAiF8KLr2Q"
      },
      "source": [
        "## CUDA Kernel: Select Action and Execute Step\n",
        "\n",
        "This kernel is a bit involved as it's actually two kernels put together - a kernel implementing the argmax action selection strategy and a kernel to execute the action and update the game state. I chose to fuse them as 1) both have the same thread grid signature (n_grids = N_GAMES, n_threads = 8) and 2) the output of the action selection kernel immediately fed into the step execution kernel. This results in removing an unnecessary array allocation and reduces the number of kernel invocations.\n",
        "\n",
        "### Argmax reduction and thread block shared memory\n",
        "\n",
        "We don't have access to an out-of-the-box argmax function with Numba, so we need to roll our own. We take advantage of CUDA's shared memory feature which allows faster access to shared memory within a thread block. The approach here is to use all 8 threads to scan along the rows of the `action_probs` array and collect both the max value and max value index, then store those in shared memory. We then need to sync the threads to make sure they are all done before continuing. Next, we use just one thread to scan over these results and pick the max of the maxes. Once we get our final x and y position for our action, we store that in shared memory. We have another thread sync barrier to make sure the threads not computing the final action wait until it's available.\n",
        "\n",
        "### Game state update\n",
        "\n",
        "Finally, each thread retrieves the action. In the event no action is possible, we assign the action a value of (-1,-1) and exit out, but not before indicating the `game_status` array that the player had to pass (0 = played, 1 = passed). If we can play, each thread evaluates one ray cast and updates the board accordingly. Finally, exit out, setting the `game_status` to played."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-mk8NjYRIBu"
      },
      "source": [
        "@cuda.jit\n",
        "def select_and_step(action_probs, boards, player, player_status):\n",
        "  rays = cuda.const.array_like(RAYS)\n",
        "  \n",
        "  row_max = cuda.shared.array(shape=(8,), dtype=float32)\n",
        "  row_max_idx = cuda.shared.array(shape=(8,), dtype=int32)\n",
        "  actions = cuda.shared.array(shape=(2,), dtype=int32)\n",
        "  \n",
        "  tx = cuda.threadIdx.x  \n",
        "  bx = cuda.blockIdx.x\n",
        "\n",
        "  opponent = 3 - player\n",
        "\n",
        "  # argmax reduction part 1\n",
        "  current_max = 0\n",
        "  current_idx = 0\n",
        "  for i in range(8):\n",
        "    if action_probs[bx, tx, i] > current_max:\n",
        "      current_max = action_probs[bx, tx, i]\n",
        "      current_idx = i\n",
        "  \n",
        "  row_max[tx] = current_max\n",
        "  row_max_idx[tx] = current_idx\n",
        "  cuda.syncthreads()\n",
        "  \n",
        "  # argmax reduction part 2\n",
        "  if tx == 0:\n",
        "    current_max = 0\n",
        "    current_idx = 0\n",
        "    for i in range(8):\n",
        "      if row_max[i] > current_max:\n",
        "        current_max = row_max[i]\n",
        "        current_idx = i\n",
        "    \n",
        "    x_idx = -1\n",
        "    y_idx = -1\n",
        "    if current_max > 0:\n",
        "      x_idx = current_idx\n",
        "      y_idx = row_max_idx[current_idx]\n",
        "    \n",
        "    actions[0] = x_idx\n",
        "    actions[1] = y_idx\n",
        "  cuda.syncthreads()\n",
        "\n",
        "  # execute action and update game state\n",
        "  act_x = actions[0]\n",
        "  act_y = actions[1]\n",
        "\n",
        "  if act_x == -1:\n",
        "    if tx == 0:\n",
        "      player_status[bx][player-1] = 1\n",
        "    return\n",
        "\n",
        "  r = rays[tx]\n",
        "  hit = ray_cast(boards[bx], act_x, act_y, r, player)\n",
        "  if hit:\n",
        "    x = act_x + r[0]\n",
        "    y = act_y + r[1]\n",
        "    while boards[bx, x, y] == opponent:\n",
        "      boards[bx, x, y] = player\n",
        "      x += r[0]\n",
        "      y += r[1]\n",
        "  \n",
        "  if tx == 0:\n",
        "    boards[bx, act_x, act_y] = player    \n",
        "    player_status[bx][player-1] = 0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGSvPzXHRMUW"
      },
      "source": [
        "In addition to the kernels and device functions, we need to allocate arrays on the device for processing and storage. Here we have two for tracking the states of all the games (board configuration and player status) and one for handling RNG. We also have our intermediate storage for our action probabilities. These all get allocated directly on the device."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJchKTd6NYVW"
      },
      "source": [
        "# state\n",
        "boards = cuda.device_array(shape=(N_GAMES, 8, 8), dtype=np.float32)\n",
        "player_status = cuda.device_array(shape=(N_GAMES, 2), dtype=np.int32)\n",
        "rng_states = create_xoroshiro128p_states(8*8*N_GAMES, seed=1)\n",
        "\n",
        "# intermediates\n",
        "action_probs = cuda.device_array(shape=(N_GAMES, 8, 8), dtype=np.float32)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hAp18cuSIqP"
      },
      "source": [
        "## Rollouts: Putting it all together\n",
        "\n",
        "Finally, we have our rollout function which handles moving data on and off the device, invoking the kernels, and deciding when we're finished. This method takes in two values: the current state of the game board and which player's turn is next. We then make `N_GAMES` copies of the board and load it into our device array along with initializing player status. We then run all games to completion, which we infer by checking that both payers have passed in all games. This does mean that we are continuing to play games that have ended, but 1) the state is effectively frozen so the outcome will never change and 2) the cost of that extra computation is negligible in this parallel scenario. We then run our two kernels and swap players. Note also that we do have to copy this data from the GPU every turn in order to check it.\n",
        "\n",
        "Finally, once all games have ended, we copy the data back to the host, count up each player's tiles and identify the winner, while accounting for possible ties.\n",
        "\n",
        "While not implemented here, it is straightforward to pull off the game states and action probabilities if there was a need to retain the history of the games as well without incurring too much penalty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoyikUpJNa0h"
      },
      "source": [
        "def rollout(board, planner):  \n",
        "  cuda.to_device(np.tile(board, (N_GAMES, 1, 1)), to=boards)\n",
        "  cuda.to_device(np.zeros((N_GAMES, 2), dtype=np.int32), to=player_status)\n",
        "\n",
        "  player = planner\n",
        "\n",
        "  while np.sum(player_status.copy_to_host()) < 2*N_GAMES:\n",
        "    find_actions[N_GAMES, (8,8)](rng_states, boards, player, action_probs)    \n",
        "    select_and_step[N_GAMES, 8](action_probs, boards, player, player_status)    \n",
        "    player = 3 - player\n",
        "\n",
        "  results = boards.copy_to_host().astype(int)\n",
        "  scores = np.array([np.bincount(b.flatten(), minlength=3)[1:] \n",
        "                     for b in results])\n",
        "  winner = np.argmax(scores, axis=1) + 1\n",
        "  winner[scores[:,0] == scores[:,1]] = 0\n",
        "\n",
        "  return winner"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi-kRyZ6UT2U"
      },
      "source": [
        "Here we can try running the actual rollouts. The first time this is run, there is a slight delay as the code gets JIT compiled. Even so, it is very fast and doesn't see much difference in timing between 32 games and 32k games!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o41Y6Hy8NllE"
      },
      "source": [
        "# initial board layout\n",
        "board = np.zeros((8, 8), dtype=np.float32)\n",
        "board[[3, 4], [3, 4]] = 2\n",
        "board[[3, 4], [4, 3]] = 1\n",
        "\n",
        "winner = rollout(board, 1)\n",
        "winner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0gUZ4UHWooi"
      },
      "source": [
        "### Was that action probability strategy really necessary?\n",
        "\n",
        "Honestly, given the small size of the task, it would have been way simpler to have just pulled the valid action array off the GPU and then used numpy to randomly select one of them. This could have then been provided as an argument to the state execution step, and likely would have not seen a major drop in performance (I haven't checked). However, that wasn't as fun an exercise as it wouldn't have required working with RNG generation, shared memory, and solving the action selection problem in parallel ;)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnRzAuNWX6O1"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}